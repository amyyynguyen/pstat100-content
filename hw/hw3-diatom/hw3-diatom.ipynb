{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw3-diatom.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSTAT 100 Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: diatoms and paleoclimatology\n",
    "\n",
    "Diatoms are a type of phytoplankton -- they are photosynthetic algae that function as primary producers in aquatic ecosystems. Diatoms are at the bottom of the food web: they are consumed by filter feeders, like clams, mussels, and many fish, which are in turn consumed by larger organisms like scavengers and predators and, well, us. As a result, changes in the composition of diatom species in marine ecosystems have ripple effects that can dramatically alter overall community structure in any environment of which marine life forms a part. \n",
    "\n",
    "Diatoms have glass bodies. As a group of organisms, they display a great diversity of body shapes, and many are quite elaborate. The image below, taken from a Scientific American article, shows a small sample of their shapes and structures.\n",
    "\n",
    "<center><img src='figures/diatoms-sciam.jpg' style='width:400px'></center>\n",
    "\n",
    "Because they are made of glass, diatoms preserve extraordinarily well over time. When they die, their bodies sink and form part of the sediment. Due to their abundance, there is a sort of steady rain of diatoms forming part of the sedimentation process, which produces sediment layers that are dense with diatoms.\n",
    "\n",
    "Sedimentation is a long-term process spanning great stretches of time, and the deeper one looks in sediment, the older the material. Since diatoms are present in high density throughout sedimentation layers, and they preserve so well, it is possible to study their presence over longer time spans -- potentially hundreds of thousands of years. \n",
    "\n",
    "A branch of paleoclimatology is dedicated to studying changes in biological productivity on geologic time scales, and much research in this area has involved studying the relative abundances of diatoms. In this assignment, you'll do just that on a small scale and work with data from sediment cores taken in the gulf of California at the location indicated on the map:\n",
    "\n",
    "<center><img src='figures/site1-barron2004.png' style='width:400px'></center>\n",
    "\n",
    "The data is publicly available: \n",
    "> Barron, J.A., *et al.* 2005. High Resolution Guaymas Basin Geochemical, Diatom, and Silicoflagellate Data. IGBP PAGES/World Data Center for Paleoclimatology Data Contribution Series # 2005-022. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\n",
    "\n",
    "---\n",
    "## Assignment objectives\n",
    "\n",
    "In this assignment, you'll use the exploratory techniques we've been discussing in class to analyze the relative abundances of diatom taxa over a time span of 15,000 years. This will involve practicing the following skills.\n",
    "\n",
    "**Acquaint and tidy**\n",
    "* data import\n",
    "* handling NaNs \n",
    "* transforming values\n",
    "* assessing time resolution\n",
    "\n",
    "**Exploratory analysis of individual variables**\n",
    "* visualizing summary statistics\n",
    "* density histograms and kernel density estimates\n",
    "* describing variation\n",
    "\n",
    "**Exploratory analysis of multiple variables**\n",
    "* examining correlation structure\n",
    "* computing and selecting principal components\n",
    "* interpreting principal component loadings\n",
    "* using principal components to visualize multivariate data\n",
    "\n",
    "**Communication and critical thinking**\n",
    "* summarizing results in written form\n",
    "* suggesting next steps\n",
    "\n",
    "Have fun!\n",
    "\n",
    "---\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "You are encouraged to collaborate with other students on the labs, but are expected to write up your own work for submission. Copying and pasting others' solutions is considered plaigarism and may result in penalties, depending on severity and extent. \n",
    "\n",
    "If you choose to work with others, please list their names here.\n",
    "\n",
    "**Your name:**\n",
    "\n",
    "**Collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Getting acquainted with the diatom data\n",
    "\n",
    "In this assignment you'll focus less on tidying and more on exploration -- the data you'll work with are already tidy. So, in this initial part, you'll:\n",
    "* import the data;\n",
    "* examine its structure to get acquainted; and \n",
    "* perform some simple preprocessing transformations to facilitate exploratory analysis. \n",
    "\n",
    "The data are diatom counts sampled from evenly-spaced depths in a sediment core from the gulf of California. In sediment cores, depth correlates with time before the present -- deeper layers are older -- and depths are typically chosen to obtain a desired temporal resolution. The counts were recorded by sampling material from sediment cores at each depth, and examining the sampled material for phytoplankton cells. For each sample, phytoplankton were identified at the taxon level and counts of diatom taxa were recorded along with the total number of phytoplankton cells identified. Thus:\n",
    "* The **observational units** are _**sediment samples**_.  \n",
    "* The **variables** are _**depth (age), diatom abundance counts, and the total number of identified phytoplankton**_. Age is inferred from radiocarbon. \n",
    "* One **observation** is made at _**each depth**_ from 0cm (surface) to  13.71 cm. \n",
    "\n",
    "The table below provides variable descriptions and units for each column in the dataframe.\n",
    "\n",
    "Variable | Description | Units\n",
    "---|---|---\n",
    "Depth | Depth interval location of sampled material in sediment core | Centimeters (cm)\n",
    "Age | Radiocarbon age | Thousands of years before present (KyrBP)\n",
    "A_curv | Abundance of *Actinocyclus curvatulus* | Count (n)\n",
    "A_octon | Abundance of *Actinocyclus octonarius* | Count (n)\n",
    "ActinSpp | Abundance of *Actinoptychus* species | Count (n)\n",
    "A_nodul | Abundance of *Azpeitia nodulifer* | Count (n)\n",
    "CocsinSpp | Abundance of *Coscinodiscus* species | Count (n)\n",
    "CyclotSpp | Abundance of *Cyclotella* species | Count (n)\n",
    "Rop_tess | Abundance of *Roperia tesselata* | Count (n)\n",
    "StephanSpp | Abundance of *Stephanopyxis* species | Count (n)\n",
    "Num.counted | Number of diatoms counted in sample | Count (n)\n",
    "\n",
    "The cell below imports the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import diatom data\n",
    "diatoms_raw = pd.read_csv('data/barron-diatoms.csv')\n",
    "diatoms_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are already in tidy format, because each row is an observation (a set of measurements on one sample of sediment) and each column is a variable (one of age, depth, or counts). However, examine rows 3 and 4. These rows illustrate two noteworthy features of the raw data:\n",
    "\n",
    "1. NaNs are present\n",
    "2. The number of individuals counted in each sample varies by a lot from sample to sample.\n",
    "\n",
    "Let's address those before conducting initial explorations.\n",
    "\n",
    "### 'Missing' values\n",
    "\n",
    "The NaNs are an artefact of the data recording -- if *no* diatoms in a particular taxa are observed, a `-` is entered in the table (you can verify this by checking the .csv file). In these cases the value isn't missing, but rather zero. These entries are parsed by pandas as NaNs, but they correspond to a value of 0 (no diatoms observed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q0 (a). Filling NaNs\n",
    "\n",
    "Use `.fill_na()` to replace all NaNs by zeros, and store the result as `diatoms_mod1`. Store rows 4 and 5 (index, not integer location) of the resulting dataframe as `diatoms_mod1_sample` and print it out.\n",
    "\n",
    "(*Hint*: check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) for `fill_na()`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diatoms_mod1 = ...\n",
    "\n",
    "# print rows 4 and 5\n",
    "diatoms_mod1_sample = ...\n",
    "print(diatoms_mod1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying total counts\n",
    "\n",
    "Since the total number of phytoplankton counted in each sample varies, the raw counts are not directly comparable -- *e.g.*, a count of 18 is actually a *different* abundance in a sample with 200 individuals counted than in a sample with 300 individuals counted.\n",
    "\n",
    "For exploratory analysis, you'll want the values to be comparable across rows. This can be achieved by a simple transformation so that the values are *relative* abundances: *proportions* of phytoplankton observed from each taxon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q0 (b). Counts to proportions\n",
    "\n",
    "Convert the counts to proportions by dividing by the relevant entry in the `Num.counted` column. There are a few ways to do this, but here's one approach:\n",
    "\n",
    "1. Set Depth and Age to row indices using `.set_index(...)` and store the result as `diatoms_mod2`.\n",
    "2. Store the `Num.counted` column from `diatoms_mod2` as `sampsize`.\n",
    "3. Use `.div(...)` to divide entrywise every column in `diatoms_mod2` by `sampsize` and store the result as `diatoms_mod3`.\n",
    "4. Drop the `Num.counted` column from `diatoms_mod3` and reset the index; store the result as `diatoms`.\n",
    "\n",
    "Carry out these steps and print the first four rows of `diatoms`.\n",
    "\n",
    "(*Hint*: careful with the `axis = ...` argument in `.div(...)`; you may want to look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.div.html).)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set depth, age to indices\n",
    "diatoms_mod2 = ...\n",
    "\n",
    "# store sample sizes\n",
    "sampsize = ...\n",
    "\n",
    "# divide\n",
    "diatoms_mod3 = ...\n",
    "\n",
    "# drop num.counted and reset index\n",
    "diatoms = ...\n",
    "\n",
    "# print\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are ready for exploratory analysis, take a moment to think about what the data represent. They are relative abundances over time; essentially, snapshots of the community composition of diatoms over time, and thus information about how ecological community composition changes.\n",
    "\n",
    "Before diving in, it will be helpful to resolve two matters:\n",
    "\n",
    "1. How far back in time do the data go?\n",
    "2. What is the time resolution of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q0 (c). Time span\n",
    "\n",
    "What is the geological time span covered by the data? Compute the minimum and maximum age using `.aggregate(...)` and store as `min_max_age`. \n",
    "\n",
    "*Note*: This may be a new function for you, but it's simple: it takes as an argument a list of functions that will be applied to the dataframe (columnwise by default). So for example, to get the mean and variance of each column in `df`, one would use `df.aggregate(['mean', 'var'])`. See the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html) for further examples.\n",
    "\n",
    "(*Remember*: age is reported as thousands of years before present, so `Age = 2` means 2000 years ago.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max_age = ...\n",
    "min_max_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q0 (d). Time resolution\n",
    "\n",
    "How are the observations spaced in time? \n",
    "\n",
    "#### (i) Make a histogram of the time steps between consecutive sample ages. \n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. Extract the `Age` column from `diatoms`, sort the values in ascending order, compute the differences between consecutive rows, and store the result as `diffs`.\n",
    "    + *Hint*: use `.sort_values()` and `.diff()`.\n",
    "    + *Notice*: that the first difference is NaN, because there is no previous value to compare the first row with. Drop this entry when you store `diffs`.\n",
    "\n",
    "\n",
    "2. Make a simple count histogram (no need to manually bin or convert to density scale) with bins of width 0.02 (20 years).\n",
    "    + Label the x axis 'Time step between consecutive sample ages'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store differences\n",
    "diffs = ...\n",
    "\n",
    "# construct histogram\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_d_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) What is the typical time step in years?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## 1. Exploring diatom taxon abundances\n",
    "\n",
    "Recall that the first type of exploratory analysis question has to do with exploring variation in each variable; to begin, you'll examine the variation in relative abundance over time for the eight individual taxa. \n",
    "\n",
    "Here are some initial questions in this spirit that will help you to hone in and develop more focuesed exploratory questions:\n",
    "* Which taxa are most and least abundant on average over time?\n",
    "* Which taxa vary the most over time?\n",
    "\n",
    "These can be answered by computing simple summary statistics for each column in the diatom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (a). Summary statistics\n",
    "\n",
    "Use `.aggregate(...)` to find the mean and standard deviation of relative abundances for each taxon. Follow these steps:\n",
    "\n",
    "1. See Q0 (c) for an explanation of `.aggregate(...)`.\n",
    "2. Drop the depth and age variables before performing the aggregation.\n",
    "3. Use `.transpose()` to ensure that the table is rendered in long form (8 rows by 2 columns rather than 2 columns by 8 rows). \n",
    "4. Store the result as `diatom_summary` and print the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diatom_summary = ...\n",
    "\n",
    "# print the dataframe\n",
    "diatom_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to determine which taxa are most/least abundant and most variable by displaying this information visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (b). Visualizing summary statistics\n",
    "\n",
    "Create a plot of the average relative abundances and their variation over time by following these steps:\n",
    "\n",
    "1. Reset the index of `diatom_summary` so that the taxon names are stored as a column and not an index. Store the result as `plot_df`.\n",
    "\n",
    "2. Create an Altair chart based on `plot_df` with *no marks* -- just `alt.Chart(...).encode(...)` -- and pass the columnn of taxon names to the `Y` encoding channel with the title 'Taxon' and sorted in descending order of mean relative abundance. Store the result as `base`.\n",
    "    + *Hint*: `alt.Y(..., sort = {'field': 'column', 'order': 'descending'})` will sort the Y channel by 'column' in descending order.\n",
    "\n",
    "\n",
    "3. Modify `base` to create a point plot of the average relative abundances for each taxon; store the result as `means`.\n",
    "    + Average relative abundance (the mean you calculated in Q1 (a)) should appear on the x axis, and taxon on the y axis.\n",
    "    + Since the `Y` encoding was already specified in `base`, you do not need to add a `Y` encoding at this stage.\n",
    "    + Give the x axis the title 'Average relative abundance'.\n",
    "\n",
    "\n",
    "4. Modify `base` to create a plot with bars spanning two standard deviations in either direction from the mean. Store the result as `bars`.\n",
    "    + First use `base.transform_calculate(...)` to compute `lwr` and `upr` for the positions of the bar endpoints:\n",
    "        - $\\texttt{lwr} = \\texttt{mean} - 2\\times\\texttt{std}$\n",
    "        - $\\texttt{upr} = \\texttt{mean} + 2\\times\\texttt{std}$.\n",
    "    + Then append `.mark_errorbar().encode(...)` to the chain:\n",
    "        - pass `lwr:Q` to the `X` encoding channel with the title 'Average relative abundance' (to match the point plot)\n",
    "        - pass `upr:Q` to the `X2` encoding channel (no specific title needed).\n",
    "\n",
    "\n",
    "5. Layer the plots: `means + bars`.\n",
    "\n",
    "\n",
    "It may help to have a look at [this example](https://altair-viz.github.io/gallery/simple_scatter_with_errorbars.html). Once you make the plot, answer questions (i) - (iii) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "\n",
    "# create base chart\n",
    "\n",
    "# create point plot\n",
    "\n",
    "# create bar plot\n",
    "\n",
    "# layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (i) Which taxon is most abundant on average over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Which taxon is most rare on average over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) Which taxon varies most in relative abundance over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now that you have a sense of the typical abundances for each taxon (measured by means) and the variations in abundance (measured by standard deviations), you'll dig in a bit further and examine the variation in abundance of the most variable taxon. \n",
    "\n",
    "For the next few questions, it may help you to follow code examples from lab 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q1 (c). Distribution of *Azpeitia nodulifer* abundance over time\n",
    "\n",
    "Here you'll construct a few plots that will help you answer the following key exploratory questions:\n",
    "* Which values are common?\n",
    "* Which values are rare?\n",
    "* How spread out are the values?\n",
    "* Are values spread evenly or irregularly?\n",
    "\n",
    "#### (i) Construct a density scale histogram of the relative abundances of *Azpeitia nodulifer*. \n",
    "\n",
    "Use the `diatoms` dataframe and a bin width of 0.03 and store the histogram as `hist`.\n",
    "\n",
    "Hint: It may help to look at *Q0* in **Lab4 on Smoothing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Construct a kernel density estimate of the distribution.\n",
    "\n",
    "Create and store the KDE curve as `smooth` and layer it on top of the density histogram from part i.\n",
    "\n",
    "(*Remember*: experiment with the bandwidth parameter, and find a value that you feel captures the shape best.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) Which values are common?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iv) Which values are rare?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (v) How spread out are the values and how are they spread out?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (vi) How would you describe the shape?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Comment: 'zero inflation'\n",
    "\n",
    "There are a disproportionately large number of zeroes, because in many samples no *Azpeitia nodulifer* diatoms were observed. This is a common phenomenon in ecological data, and even has a name: it results in a 'zero inflated' distribution of values. The statistician to identify and name the phenomenon was Diane Lambert, whose highly influential work on the subject (>4k citations) was published in 1992.\n",
    "\n",
    "Zero inflation can present a variety of challenges. You may have noticed, for example, that there was no bandwidth parameter for the KDE curve that *both* captured the shape of the histogram near zero *and* away from zero -- it either got the height near zero right but was too wiggly, or got the shape away from zero right but was too low near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning on a climate event\n",
    "\n",
    "There was a major climate event during the time span covered by the diatom data. The oldest data points in the diatom data correspond to the end of the Pleistocene epoch (ice age), at which time there was a pronounced warming (Late Glacial Interstadial, 14.7 - 12.9 KyrBP) followed by a return to glacial conditions (Younger Dryas, 12.9 - 11.7 KyrBP). \n",
    "\n",
    "This fluctuation can be seen from temperature reconstructions. Below is a plot of sea surface temperature reconstructions off the coast of Northern California. Data come from the following source:\n",
    "\n",
    "> Barron *et al.*, 2003. Northern Coastal California High Resolution Holocene/Late Pleistocene Oceanographic Data. IGBP PAGES/World Data Center for Paleoclimatology. Data Contribution Series # 2003-014. NOAA/NGDC Paleoclimatology Program, Boulder CO, USA.\n",
    "\n",
    "The shaded region indicates the time window with unusually large flucutations in sea surface temperature; this window roughly corresponds to the dates of the climate event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sea surface temp reconstruction\n",
    "seatemps = pd.read_csv('data/barron-sst.csv')\n",
    "\n",
    "# line plot of time series\n",
    "line = alt.Chart(seatemps).mark_line().encode(\n",
    "    x = alt.X('Age', title = 'Thousands of years before present'),\n",
    "    y = 'SST'\n",
    ")\n",
    "\n",
    "# highlight region with large variations\n",
    "highlight = alt.Chart(\n",
    "    pd.DataFrame(\n",
    "        {'SST': np.linspace(0, 14, 100), \n",
    "         'upr': np.repeat(11, 100), \n",
    "         'lwr': np.repeat(15, 100)}\n",
    "    )\n",
    ").mark_area(opacity = 0.2, color = 'orange').encode(\n",
    "    y = 'SST',\n",
    "    x = alt.X('upr', title = 'Thousands of years before present'),\n",
    "    x2 = 'lwr'\n",
    ")\n",
    "\n",
    "# add smooth trend\n",
    "smooth = line.transform_loess(\n",
    "    on = 'Age',\n",
    "    loess = 'SST',\n",
    "    bandwidth = 0.2\n",
    ").mark_line(color = 'black')\n",
    "\n",
    "# layer\n",
    "line + highlight + smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q1 (d). Conditional distributions of relative abundance\n",
    "\n",
    "Does the distribution of relative abundance of *Azpeitia nodulifer* differ when variation in sea temperatures was higher (before 11KyrBP)? \n",
    "\n",
    "#### (i) Plot kernel density estimates to show the distribution of relative abundances before and after 11KyrBP.\n",
    "\n",
    "1. Use `.transform_caluculate(...)` to calculate an indicator variable, `pre_dryas`, that indicates whether `Age` exceeds 11.\n",
    "\n",
    "2. Use `.transform_density(...)` to compute KDEs separately for observations of relative abundance before and after 11KyrBP.\n",
    "    + *Hint*: group by `pre_dryas`\n",
    "\n",
    "\n",
    "3. Plot the KDEs distinguished by color; give the color legend the title 'Before 11KyrBP' and store the plot as `kdes`.\n",
    "\n",
    "4. Add a shaded area beneath the KDE curves. Adjust the opacity of the area to your liking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Describe the variation in relative abundance of *Azpeitia nodulifer* between now and 11,000 years ago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) Describe the variation in relative abundance of *Azpeitia nodulifer* between 11,000 and 14,000 years ago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Visualizing community composition with PCA\n",
    "\n",
    "So far you've seen that the abundances of one taxon -- *Azpeitia nodulifer* -- change markedly before and after a shift in climate conditions. In this part you'll use PCA to explore variation in community composition *among* all eight taxa.\n",
    "\n",
    "Throughout this part, it may help you to refer to code examples from lab 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (a). Pairwise correlations in relative abundances\n",
    "\n",
    "Before carrying out PCA it is a good idea to inspect the correlations between relative abundances directly. Here you'll compute and then visualize the correlation matrix.\n",
    "\n",
    "#### (i) Compute the pairwise correlations between relative abundances.\n",
    "\n",
    "Be sure to remove or set to indices the Depth and Age variables before computing the correlation matrix. Save the matrix as `corr_mx` and print the result. \n",
    "\n",
    "*Hint:* See the [pandas documentation for `.corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_mx = ...\n",
    "corr_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_a_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Visualize the correlation matrix as a heatmap\n",
    "\n",
    "Have a look at either lab 5 or [this example](https://altair-viz.github.io/gallery/simple_heatmap.html) (or both!). Notice that to make a heatmap of a matrix, you'll need to melt it into long format.\n",
    "\n",
    "1. Melt `corr_mx` to obtain a dataframe with three columns: \n",
    "    + `row`, which contains the values of the index of `corr_mx` (taxon names); \n",
    "    + `column`, which contains the names of the columns of `corr_mx` (also taxon names); and\n",
    "    + `Correlation`, which contains the values of `corr_mx`. \n",
    "    \n",
    "    **Store the result as `corr_mx_long`.**\n",
    "\n",
    "\n",
    "2. Create an Altair chart based on `corr_mx_long` and construct the heatmap by following the examples indicated above.\n",
    "    + Adjust the color scheme to `blueorange` over the extent (-1, 1) to obtain a diverging color gradient where a correlation of zero is blank (white).\n",
    "    + Adjust the color legend to indicate the color values corresponding to correlations of 1, 0.5, 0, -0.5, and -1.\n",
    "    + Sort the rows and columns in ascending order of correlation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# melt corr_mx\n",
    "...\n",
    "\n",
    "# construct plot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_a_ii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) How does the relative abundance of *Azpeitia nodulifer* seem to covary with the other taxa?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2 (b). Computing and selecting principal components\n",
    "\n",
    "Here you'll perform all of the calculations involved in PCA and check the variance ratios to select an appropriate number of principal components. The parts of this question correspond to the individual steps in this process.\n",
    "\n",
    "#### (i) Center and scale the data columns.\n",
    "\n",
    "For PCA it is usually recommended to center and scale the data; set Depth and Age as indices and center and scale the relative abundances. Store the normalized result as `pcdata`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper variable pcdata_raw; set Depth and Age as indices\n",
    "pcdata_raw = ...\n",
    "\n",
    "# center and scale the relative abundances\n",
    "pcdata = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_b_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Compute the principal components.\n",
    "\n",
    "Compute *all 8* principal components. (For this part you do not need to show any specific output.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_b_ii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Examine the variance ratios.\n",
    "\n",
    "Create a dataframe called `pcvars` with the variance information by following these steps:\n",
    "\n",
    "1. Store the proportion of variance explained (called `.explained_variance_ratio_` in the PCA output) as a dataframe named `pcvars` with just one column named `Proportion of variance explained`.\n",
    "2. Add a column named `Component` to `pcvars` with the integers 1 through 8 as values (indicating the component number).\n",
    "3. Add a column named `Cumulative variance explained` to `pcvars` that is the cumulative sum of `Proportion of variance explained`.\n",
    "    + *Hint*: slice the `Proportion of variance explained` column and use `.cumsum(axis = ...)`.\n",
    "\n",
    "\n",
    "Print the dataframe `pcvars`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store proportion of variance explained as a dataframe\n",
    "pcvars = ...\n",
    "\n",
    "# add component number as a new column\n",
    "...\n",
    "\n",
    "# add cumulative variance explained as a new column\n",
    "...\n",
    "\n",
    "# print\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_b_iii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iv) Plot the variance explained by each PC.\n",
    "\n",
    "Use `pcvars` to construct a dual-axis plot showing the proportion of variance explained (left y axis) and cumulative variance explained (right y axis) as a function of component number (x axis), with points indicating the variance ratios and lines connecting the points. \n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. Construct a base chart that encodes only `Component` on the `X` channel. Store this as `base`.\n",
    "2. Make a base layer for the proportion of variance explained that modifies `base` by encoding `Proportion of variance explained` on the `Y` channel. Store the result as `prop_var_base`.\n",
    "    + Give the `Y` axis title a distinct color of your choosing via `alt.Y(..., axis = alt.Axis(titleColor = ...))`.\n",
    "\n",
    "\n",
    "3. Make a base layer for the cumulative variance explained that modifies `base` by endocing `Cumulative variance explained` on the `Y` channel. Store the result as `cum_var_base`.\n",
    "    + Give the `Y` axis title another distinct color of your choosing via `alt.Y(..., axis = alt.Axis(titleColor = ...))`.\n",
    "\n",
    "\n",
    "4. Create a plot layer for the proportion of variance explained by combining points (`prop_var_base.mark_point()`) with lines (`prop_var_base.mark_line()`). Store the result as `cum_var`.\n",
    "    + Apply the color you chose for the axis title to the points and lines.\n",
    "\n",
    "\n",
    "5. Repeat the previous step for the cumulative variance explained.\n",
    "    + Apply the color you chose for the axis title to the points and lines.\n",
    "\n",
    "\n",
    "6. Layer the plots together using `alt.layer(l1, l2).resolve_scale(y = 'independent')`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode component axis only as base layer\n",
    "base = ...\n",
    "\n",
    "# make a base layer for the proportion of variance explained\n",
    "...\n",
    "\n",
    "# make a base layer for the cumulative variance explained\n",
    "...\n",
    "\n",
    "# add points and lines to each base layer\n",
    "...\n",
    "\n",
    "# layer the layers\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (v) How many PCs?\n",
    "\n",
    "How many principal components capture a high proportion of covariation in relative abundances? How much total variation do these explain together?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2 (c). Interpreting component loadings\n",
    "\n",
    "Now that you've performed the calculations for PCA, you can move on to the fun part: figuring out what they say about the data!\n",
    "\n",
    "The first step in this process is to examine the loadings. Each principal component is a linear combination of the relative abundances by taxon, and the loadings tell you *how* that combination is formed; the loadings are the linear combination coefficients, and thus correspond to the weight of each taxon in the corresponding principal component. Some useful points to keep in mind:\n",
    "* a high loading value (negative or positive) indicates that a variable strongly influences the principal component;\n",
    "* a negative loading value indicates that\n",
    "    + increases in the value of a variable *decrease* the value of the principal component \n",
    "    + and decreases in the value of a variable *increase* the value of the principal component;\n",
    "* a positive loading value indicates that \n",
    "    + increases in the value of a variable *increase* the value of the principal component\n",
    "    + and decreases in the value of a variable *decrease* the value of the principal component;\n",
    "* similar loadings between two or more variables indicate that the principal component reflects their *average*;\n",
    "* divergent loadings between two sets of variables indicates that the principal component reflects their *difference*.\n",
    "\n",
    "#### (i) Extract the loadings from `pca`.\n",
    "\n",
    "Store the loadings for the first two principal components (called `.components_` in the PCA output) in a dataframe named `loading_df`. Name the columns `PC1` and `PC2`, and append a column `Taxon` with the corresponding variable names, and print the resulting dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store the loadings as a data frame with appropriate names\n",
    "...\n",
    "\n",
    "# add a column with the taxon names\n",
    "...\n",
    "\n",
    "# print\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_c_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Loading plots\n",
    "\n",
    "Construct a line-and-point plot connecting the loadings of the first two principal components. Display the value of the loading on the y axis and the taxa names on the x axis, and show points indicating the loading values. Distinguish the PC's by color, and add lines connecting the loading values for each principal component.\n",
    "\n",
    "*Hint*: you will need to first melt `loading_df` to long form with three columns -- the taxon name, the principal component (1 or 2), and the value of the loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# melt from wide to long\n",
    "...\n",
    "\n",
    "# create base layer with encoding\n",
    "...\n",
    "\n",
    "# store horizontal line at zero\n",
    "rule = ...\n",
    "\n",
    "# layer points + lines + rule to construct loading plot\n",
    "loading_plot = ...\n",
    "\n",
    "# show\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) Interpret the first principal component.\n",
    "\n",
    "Indicate the following:\n",
    "* which taxa are up-weighted and which are down-weighted;\n",
    "* how you would describe the principal component in context (*e.g.*, average abundance among a group, differences in abundances, etc.);\n",
    "* and how you would interpret a positive value of the PC versus a negative value of the PC in terms of diatom communnity composition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iv) Interpret the second principal component.\n",
    "\n",
    "Indicate the following:\n",
    "* which taxa are up-weighted and which are down-weighted;\n",
    "* how you would describe the principal component in context (*e.g.*, average abundance among a group, differences in abundances, etc.);\n",
    "* and how you would interpret a positive value of the PC versus a negative value of the PC in terms of diatom community composition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2 (d). Visualizing community composition\n",
    "\n",
    "Take a moment to recall that there was a shift in the *A. nodulifer* abundance before and after around 11,000 years ago, which roughly corresponded to a major transition in the earth's climate. \n",
    "\n",
    "Well, you can now use PCA to investigate whether not just individual abundances but *community composition* may have shifted around that time. To that end, let's think of the principal components as 'community composition indices':\n",
    "* consider PC1 a nodulifer/non-nodulifer community composition index; and \n",
    "* consider PC2 a complex community composition index. \n",
    "\n",
    "A pattern of variation or covariation in the principal components can be thought of as reflecting a particular ecological community composition dynamic -- a way that community composition varies throughout time. Here you'll look for distinct patterns of variation/covariation before and after 11,000 years ago via an exploratory plot of the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Project the centered and scaled data onto the first two component directions.\n",
    "\n",
    "This sounds a little more complicated than it is -- all that means is compute the values of the principal components for each data point. \n",
    "\n",
    "Create a dataframe called `projected_data` containing just the first two principal components as two columns named `PC1` and `PC2`, and two additional columns with the Age and Depth variables. \n",
    "\n",
    "Print the first four rows of `projected_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# project pcdata onto first two components; store as data frame\n",
    "projected_data = ...\n",
    "\n",
    "# add index and reset\n",
    "projected_data.index = ...\n",
    "projected_data = ...\n",
    "\n",
    "# print first four rows\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_d_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Construct a scatterplot of PC1 and PC2 by age indicator.\n",
    "\n",
    "Follow these steps to construct a scatterplot of the principal components.\n",
    "\n",
    "1. Create an Altair chart based on `projected_data` and use `.transform_calculate(...)` to define a variable `since_11KyrBP` that indicates whether `Age` is older than 11,000 years. Store the result as `base`.\n",
    "2. Modify `base` to add points with the following encodings. \n",
    "    + Pass PC1 to the `X` encoding channel and title the axis 'A. Nodulifer/non-A. nodulifer composition'.\n",
    "    + Pass PC2 to the `Y` encoding channel and title the axis 'Complex community composition'.\n",
    "    + Pass the variable you created in step 1. to the `color` encoding channel and title it 'More recent than 11KyrBP'.\n",
    "    Store the result as `scatter`.\n",
    "    \n",
    "Show the scatterplot once you complete these steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base chart\n",
    "...\n",
    "\n",
    "# data scatter\n",
    "...\n",
    "\n",
    "# show\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (iii) Add top and side panels with KDE curves.\n",
    "\n",
    "Construct plots of kernel density estimates for each principal component conditional on age being older than 11,000 years:\n",
    "* modify `base` to create a `top_panel` plot with the KDE curves for PC1, with color corresponding to the age indicator from the `.transform_calculate(...)` step in making the base layer;\n",
    "* modify `base` again to create a `side_panel` plot with the KDE curves for PC2, rotated 90 degrees relative to the usual orientation (flip the typical axes), and with color corresponding to the age indicator from the `.transform_calculate(...)` step in making the base layer.\n",
    "\n",
    "Then, resize these panels appropriately (top should be thin, side should be narrow), and use Altair's faceting operators `&` (vertical concatenation) and `|` (horizontal concatenation) to combine them with your scatterplot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct upper panel (kdes for pc1)\n",
    "...\n",
    "\n",
    "# construct side panel (kdes for pc2)\n",
    "...\n",
    "\n",
    "# facet\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Communicating results\n",
    "\n",
    "Take a moment to review and reflect on the results of your analysis in the previous parts. Think about how you would describe succinctly what you've learned from the diatom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q3 (a). Summary\n",
    "\n",
    "Write a brief paragraph (3-5 sentences) that addresses the following questions by referring to your final plot in Q2 (d).\n",
    "* How would you characterize the typical ecological community composition of diatom taxa before and after 11,000 years ago?\n",
    "    + *Hint*: focus on the side and top panels and the typical values of each index in the two time periods.\n",
    "* Does the variation in ecological community composition over time seem to differ before and after 11,000 years ago?\n",
    "    + *Hint*: focus on the shape of data scatter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q3 (b). Further work\n",
    "\n",
    "What more might you like to know, given what you've learned? Pose a question that your exploratory analysis raises for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "*Type your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---\n",
    "## Submission Checklist\n",
    "1. Save file to confirm all changes are on disk\n",
    "2. Run *Kernel > Restart & Run All* to execute all code from top to bottom\n",
    "3. Save file again to write any new output to disk\n",
    "4. Select *File > Download as > HTML*.\n",
    "5. Open in Google Chrome and print to PDF on A3 paper in portrait orientation.\n",
    "6. Submit to Gradescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "otter": {
   "tests": {
    "q0_a": {
     "name": "q0_a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # diatoms_mod1 doesn't contain any NaN values\n>>> diatoms_mod1.isnull().values.any() == False\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # diatoms_raw.loc[4, \"StephanSpp\"] is NaN; want to confirm that diatoms_mod1 is 0 instead\n>>> diatoms_mod1.loc[4, \"StephanSpp\"] == 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # check for correct knowledge of extracting two rows from a dataframe by index\n>>> (diatoms_mod1_sample == diatoms_mod1.iloc[4:6, :]).all().all() and \\\n... (diatoms_mod1_sample == diatoms_mod1.loc[4:5, :]).all().all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q0_b": {
     "name": "q0_b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # checking that diatoms_mod2 is indexed by Depth and Age\n>>> diatoms_mod2.index.names == [\"Depth\", \"Age\"]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that the number of columns of diatoms_mod2 is 9\n>>> len(diatoms_mod2.columns) == 9\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that the first row of samp_size == 201\n>>> sampsize[0] == [201]\nAge\n1.33    True\nName: Num.counted, dtype: bool",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that for all the rows, num.counted is 1; all the other columns should be less than 1\n>>> (diatoms_mod3[\"Num.counted\"] == 1).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that all the proportions of diatoms are less than (or equal to) 1\n>>> (diatoms_mod3.loc[:,['A_curv', 'A_octon', 'ActinSpp', 'A_nodul', 'CoscinSpp', 'CyclotSpp', 'Rop_tess', 'StephanSpp']] <= 1).all().all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that once the indices have been reset to before and the Num.Counted column is dropped, we are left with 10 columns\n>>> len(diatoms.columns) == 10\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking the values of the first row of diatoms\n>>> (round(diatoms.iloc[0,:],3) ==[0., 1.33 , 0.025, 0.01 , 0.159, 0.07 , 0.104, 0.109, 0.005, 0.005]).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q0_c": {
     "name": "q0_c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # checking rounded values\n>>> (round(min_max_age) == [1,15]).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_a": {
     "name": "q1_a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (diatom_summary.columns == [\"mean\", \"std\"]).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking the shape of the dataframe\n>>> diatom_summary.shape == (8,2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_d_i": {
     "name": "q1_d_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # checking the length of diffs\n>>> len(diffs) == 229\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # testing the max difference\n>>> diffs.max() == 1.14\ndiff    False\ndtype: bool",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that NA got removed\n>>> diffs.iloc[0].isna().values[0] == False\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_a_i": {
     "name": "q2_a_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # checking the shape of the correlation matrix (do not want Age or Depth to be a component of the matrix)\n>>> corr_mx.shape == (8,8)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking that the diagonals are 1s (the correlation between a variable and itself should be 1)\n>>> (np.diag(corr_mx) == np.ones(8)).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_a_ii": {
     "name": "q2_a_ii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> corr_mx_long.shape == (64,3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (corr_mx_long.columns == ['row', 'col', 'Correlation']).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_b_i": {
     "name": "q2_b_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> #checking the shape of pcdata as we are retaining Depth and Age as indices\n>>> pcdata.shape == (230, 8)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_b_ii": {
     "name": "q2_b_ii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> pca.n_components_ == 8\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_b_iii": {
     "name": "q2_b_iii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check that we have 8 rows and 3 columns\n>>> pcvars.shape == (8,3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # the summation of all the values in the Proportion of variance explained column should be 1\n>>> round(pcvars[\"Proportion of variance explained\"].sum()) == 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # the last value of the cumulative sum should be 1 (since all the values in `Proportion of variance explained` adds to 1)\n>>> round(pcvars[\"Cumulative variance explained\"].iloc[-1]) == 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_c_i": {
     "name": "q2_c_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> loading_df.shape == (8,3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # sanity check of the PC1 columns' first value\n>>> loading_df.loc[0,\"PC1\"] == pca.components_[0,0]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # sanity check of the PC2 columns' first value\n>>> loading_df.loc[0,\"PC2\"] == pca.components_[1,0]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # sanity check of the Taxon columns' first value\n>>> loading_df.loc[0,\"Taxon\"] == \"A_curv\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_d_i": {
     "name": "q2_d_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check of the columns included\n>>> (projected_data.columns == ['Depth', 'Age', 'PC1', 'PC2']).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # checking the shape of the dataframe\n>>> projected_data.shape == (230,4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # sanity check of the first row\n>>> (round(projected_data.iloc[0],3) == [0, 1.33, -0.295, -0.633]).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
